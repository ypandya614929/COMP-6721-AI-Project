{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://medium.com/swlh/introduction-to-cnn-image-classification-using-cnn-in-pytorch-11eefae6d83c\n",
    "# https://towardsdatascience.com/how-i-built-a-face-mask-detector-for-covid-19-using-pytorch-lightning-67eb3752fd61\n",
    "# https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529\n",
    "# https://core.ac.uk/reader/328808130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import ToPILImage, Compose, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of training and testing dataset\n",
    "dir_path = \"/Users/ypandya/Documents/Concordia/COMP-6721/COMP-6721-AI-Project/Dataset\"\n",
    "mask_dir = \"{}/person_with_mask\".format(dir_path)\n",
    "non_mask_dir = \"{}/person\".format(dir_path)\n",
    "not_person_dir = \"{}/not_person\".format(dir_path)\n",
    "procesed_dir = \"{}/processed_data.npy\".format(dir_path)\n",
    "save_model_name = \"ai-project-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels (classes) to differentiate the images in these categories\n",
    "label_dict = {\n",
    "    0: \"Person with Face Mask\", \n",
    "    1: \"Person without Face Mask\", \n",
    "    2: \"Not a Person\"\n",
    "}\n",
    "\n",
    "# Labels to display on the confussion matrix\n",
    "labels_list = [\"Person with Face Mask\", \"Person without Face Mask\", \"Not a Person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized dataset class named ProjectDataset which is inherited\n",
    "# from Dataset class of pytorch, the purpose of inheritance here is \n",
    "# to store the labels(keys) of images with the image ndarray this way we\n",
    "# can remove the problem of overfitting. This overfitting problem was there\n",
    "# when initially project implementation started as we have purposely devided\n",
    "# images of different lables/classes in different directories.\n",
    "class ProjectDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.transformations = Compose([\n",
    "            ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        item_list = []\n",
    "        try:\n",
    "            item_list.append(self.transformations(self.data[key][0]))\n",
    "            item_list.append(torch.tensor(self.data[key][1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return item_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized convolution neural network class which is again inherited from the torch\n",
    "# neural network class. this class contains all the filters and other operations\n",
    "# which are performed on images. The class contains forward method which is used to\n",
    "# feedforward the neural network and the backpropogation.\n",
    "class COMP_6721_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(COMP_6721_CNN, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # convolution layer 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # convolution layer 2\n",
    "            nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # convolution layer 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(36864, 3)\n",
    "        )\n",
    "\n",
    "    # forward pass to readjust weights  \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes empty data as list, directory path to find the dataset\n",
    "# location and key which used to describe the category of image (0, 1, 2) based \n",
    "# on the which class it belongs. after converting images into an array the function\n",
    "# will return all the array within list to process further.\n",
    "def load_images(data, dir_path, key):\n",
    "    print(\"=== Loading Images ===\")\n",
    "    for file_path in os.listdir(dir_path):\n",
    "        temp_img = cv2.resize(cv2.imread(\"{}/{}\".format(dir_path, file_path), cv2.IMREAD_COLOR), (100, 100))\n",
    "        data.append([np.array(temp_img), key])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes image data as a list and saving data directory path and after\n",
    "# shuffling the data, it will store numpy array at the specific directory.\n",
    "def shuffle_and_save_data(data, save_data_dir):\n",
    "    print(\"=== Shuffel Images ===\")\n",
    "    np.random.shuffle(data)\n",
    "    np.save(save_data_dir, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes process and stored data dictionary path, after reading data\n",
    "# from the given path, the function distributes data across training, testing and\n",
    "# validation data category. It combines all the data with a key and returns a \n",
    "# dictionary for the further use.\n",
    "def load_and_distribute_data(procesed_dir):\n",
    "    print(\"=== Loading and Distributing Images across Training, Testing and Validation data ===\")\n",
    "    loaded_data = np.load(procesed_dir, allow_pickle=True)\n",
    "    training, testing_data = train_test_split(loaded_data, test_size=0.05, random_state=0)\n",
    "    training_data, validation_data = train_test_split(training, test_size=0.15, random_state=0)\n",
    "    distributed_data = {\n",
    "        \"training_data\": training_data,\n",
    "        \"testing_data\": testing_data,\n",
    "        \"validation_data\": validation_data\n",
    "    }\n",
    "    return distributed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a dictionary of keys (label classes) along with the\n",
    "# data for all clasess and builds the loader for each training, testing and\n",
    "# validation category and returns it for further use.\n",
    "def build_dataset_loaders(distributed_data):\n",
    "    print(\"=== Bulding Data Loaders ===\")\n",
    "    training_dataset = ProjectDataset(distributed_data.get(\"training_data\"))\n",
    "    validation_dataset = ProjectDataset(distributed_data.get(\"validation_data\"))\n",
    "    test_dataset = ProjectDataset(distributed_data.get(\"testing_data\"))\n",
    "    train_loader = DataLoader(training_dataset, batch_size=128)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=128)\n",
    "    return train_loader, test_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes training data and returns a normalized list based on\n",
    "# number of data falls into each category/label (classes) which is used while\n",
    "# building a loss criteria.\n",
    "def normalized_weights_to_build_loss_criteria(training_data):\n",
    "    print(\"=== Normalizing weights accorss all Label classes ===\")\n",
    "    # Dict to count number of training images across the different classes\n",
    "    # for normalization purpose\n",
    "    training_data_count_dict = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0\n",
    "    }\n",
    "    normalization_values = []\n",
    "    for data in training_data:\n",
    "        training_data_count_dict.update({data[1]: training_data_count_dict.get(data[1])+1})\n",
    "    categorized_value_list = list(training_data_count_dict.values())\n",
    "    total_counts = sum(categorized_value_list)\n",
    "    for category_value in categorized_value_list:\n",
    "        key_value = (1 - (category_value / total_counts))\n",
    "        normalization_values.append(key_value)\n",
    "    return normalization_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a device and normalizeed list to build a model.\n",
    "def build_model(device, normalization):\n",
    "    print(\"=== Bulding Model ===\")\n",
    "    model = COMP_6721_CNN()\n",
    "    model = model.to(device)\n",
    "    loss_criteria = nn.CrossEntropyLoss(weight=torch.tensor(normalization).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    return model, optimizer, loss_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a model, optimizer, loss_criteria and training loader to \n",
    "# train the model, after multiple epoches, model gets stablize and function return\n",
    "# the model to use it for evaluation.\n",
    "def train_model(model, optimizer, loss_criteria, train_loader):\n",
    "    print(\"=== Training Model ===\")\n",
    "    for i in range(10):\n",
    "        model.train()\n",
    "        training_accuracy_list = []\n",
    "        training_loss_list = []\n",
    "\n",
    "        for data_chunk in train_loader:\n",
    "            images, labels = data_chunk\n",
    "            outputs = model(images)\n",
    "            training_loss = loss_criteria(outputs, labels.long())\n",
    "            _, prediction_vals = torch.max(outputs, dim=1)\n",
    "            training_accuracy = torch.tensor(torch.sum(prediction_vals == labels).item() / len(prediction_vals))\n",
    "            training_accuracy_list.append(training_accuracy)\n",
    "            training_loss_list.append(training_loss)\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(\"Epoch: {}, training loss: {}, training accuracy: {}\".format(\n",
    "            i+1, torch.stack(training_loss_list).mean().item(), torch.stack(training_accuracy_list).mean().item()\n",
    "        ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes model and model name and saves the model at given directory\n",
    "# location.\n",
    "def save_model(model, dir_path, save_model_name):\n",
    "    print(\"=== Saving Model ===\")\n",
    "    torch.save(model.state_dict(), \"{}/{}\".format(dir_path, save_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes directory and model name to load the stored model from\n",
    "# given name and location.\n",
    "def load_saved_model(dir_path, device, save_model_name):\n",
    "    print(\"=== Loading saved Model ===\")\n",
    "    loaded_model = torch.load(\"{}/{}\".format(dir_path, save_model_name), map_location=device)\n",
    "    model = COMP_6721_CNN()\n",
    "    model.load_state_dict(loaded_model)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a model follwed by title of report and matrix and the data loader \n",
    "# which is either training, testing or validation data loader. It calculates the accuracy\n",
    "# and displays the classification report based on actual and predicted label. The report\n",
    "# contains F1, Recall, Precision values and followed by Confussion matrix to compare the\n",
    "# actual and predicted results.\n",
    "def generate_classification_report_and_plot_confusion_matrix(model, title, data_loader):\n",
    "    print(\"=== Generating Classification Report ===\")\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    accurate_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_chunk in data_loader:\n",
    "            images, labels = data_chunk\n",
    "            _, pred_values = torch.max(model(images), dim=1)\n",
    "            predictions_list.extend(pred_values.detach().cpu().numpy())\n",
    "            accurate_list.extend(labels.detach().cpu().numpy())\n",
    "    print(\"{} Classification Report\".format(title))\n",
    "    print(classification_report(accurate_list, predictions_list))\n",
    "    print(\"=== Generating Confusion Matrix ===\")\n",
    "    plt.figure()\n",
    "    confusion_matrix_instance = confusion_matrix(accurate_list, predictions_list)\n",
    "    plt.imshow(confusion_matrix_instance, interpolation='nearest', cmap=plt.cm.Pastel2)\n",
    "    for (x_cordinate, y_cordinate), val in np.ndenumerate(confusion_matrix_instance):\n",
    "        plt.text(x_cordinate, y_cordinate, val, ha='center', va='center')\n",
    "    plt.title('{} Confusion matrix'.format(title))\n",
    "    plt.ylabel('Actual labels')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    randomized_val = np.arange(len(labels_list))\n",
    "    plt.xticks(randomized_val, labels_list, rotation=60)\n",
    "    plt.yticks(randomized_val, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is treated as a main function to run the program. The function takes\n",
    "# two boolean parameters is_data_preloaded and is_model_saved. If the value of is_data_preloaded\n",
    "# is True which means program reads pre-loaded data from the directory, otherwise it generates\n",
    "# the data and loads it. While the True value of is_model_saved parameter indicates that\n",
    "# program reads model from the directory which is pre stored and if the parameter is set as \n",
    "# False then it creates new model, trains the model and then it becomes available for \n",
    "# further use such as in evaluation process.\n",
    "def run_program(\n",
    "    is_data_preloaded = False, \n",
    "    is_model_saved = False\n",
    "):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    if not is_data_preloaded:\n",
    "        data = []\n",
    "        data = load_images(data, mask_dir, 0)\n",
    "        data = load_images(data, non_mask_dir, 1)\n",
    "        data = load_images(data, not_person_dir, 2)\n",
    "        shuffle_and_save_data(data, procesed_dir)\n",
    "    distributed_data = load_and_distribute_data(procesed_dir)\n",
    "    train_loader, test_loader, validation_loader = build_dataset_loaders(distributed_data)\n",
    "    normalization = normalized_weights_to_build_loss_criteria(distributed_data.get(\"training_data\"))\n",
    "    if not is_model_saved:\n",
    "        model, optimizer, loss_criteria = build_model(device, normalization)\n",
    "        model = train_model(model, optimizer, loss_criteria, train_loader)\n",
    "        save_model(model, dir_path, save_model_name)\n",
    "    model = load_saved_model(dir_path, device, save_model_name)\n",
    "    generate_classification_report_and_plot_confusion_matrix(model, \"Training\", train_loader)\n",
    "    generate_classification_report_and_plot_confusion_matrix(model, \"Validation\", validation_loader)\n",
    "    generate_classification_report_and_plot_confusion_matrix(model, \"Testing\", test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading and Distributing Images across Training, Testing and Validation data ===\n",
      "=== Bulding Data Loaders ===\n",
      "=== Normalizing weights accorss all Label classes ===\n",
      "=== Loading saved Model ===\n",
      "=== Generating Classification Report ===\n"
     ]
    }
   ],
   "source": [
    "# If the value of is_data_preloaded\n",
    "# is True which means program reads pre-loaded data from the directory, otherwise it generates\n",
    "# the data and loads it. While the True value of is_model_saved parameter indicates that\n",
    "# program reads model from the directory which is pre stored and if the parameter is set as \n",
    "# False then it creates new model, trains the model and then it becomes available for \n",
    "# further use such as in evaluation process.\n",
    "\n",
    "run_program(is_data_preloaded=True, is_model_saved=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
